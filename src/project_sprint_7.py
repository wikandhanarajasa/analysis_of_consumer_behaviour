# -*- coding: utf-8 -*-
"""Project Sprint 7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eZTwIcqUb4jcw93hk3jGD0SehJ7kgBpF

# **Analysis of Consumer Behaviour to New Products**

## **Introduction**

This project aim to have a model to predict consumer behaviour to new product at 75% of accuracy.

### **Goal**

Having a model to predict customer behavior using machine learning that have at least 75% of accuracy.

### **Stages**

The stages of this project are as below:

1. Open the data file and study its general information.

2. Prepare the data

- Explore the data and check for anomalies

3. Devide the data into three categories: Training set, Validation set, and Test set.
4. Check the model quality using by changing the hyperparameter.
5. Check the model quality using test set.
6. Run the sanity check.
7. Draw the conclusion.

### **Data Content**

Costumer behaviour consist of columns as below:

1. calls — number of calls
2. minutes — total call duration in minutes
3. messages — number of text messages
4. mb_used — internet usage traffic in MB
5. is_ultimate — package for the current month (Ultimate - 1, Surf - 0)

## **Data Loading and Initial Examination**
"""

# Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

# Loading dataset

df = pd.read_csv('/content/users_behavior.csv')
df

"""## **Data Preparation**"""

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df_eda = df.copy()
df_eda['is_ultra'] = df_eda['is_ultra'].astype(str)
df_eda

df_eda_1 = df_eda.pivot_table(index='is_ultra', values='calls', aggfunc='count').reset_index()
df_eda_1.columns = ['is_ultra', 'trx']
df_eda_1

sns.barplot(data=df_eda_1, x='is_ultra', y='trx')
plt.title('Transaction Count by Ultra Status')
plt.show()

sns.boxplot(data=df_eda, x='calls', y='is_ultra')
plt.title('Distribution of Calls by Ultra Status')
plt.show()

sns.boxplot(data=df_eda, x='messages', y='is_ultra')
plt.title('Distribution of Messages by Ultra Status')
plt.show()

sns.boxplot(data=df_eda, x='mb_used', y='is_ultra')
plt.title('Distribution of MB Used by Ultra Status')
plt.show()

"""## **Data Setting**

In order to run machine learning we have to devided the data into two, they are features and target. in this case the the features columns are everything but is_ultra, and the target column consist only is_ultra.
"""

features = df.drop('is_ultra', axis=1)
target = df['is_ultra']

features.head()

target.head()

"""Seems like the features and target well devided.

Since we have a lot of data, we will have the percentage as below:

- 60% data for training set.
- 20% data for test set.
- 20% data for validation set.

Now split each of them into train model and test model using sklearn train_test_split
"""

features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.4, random_state=12)

"""Then devide again into test and validation model using sklearn train_test_split"""

features_val, features_test, target_val, target_test = train_test_split(features_test, target_test, test_size=0.5, random_state=12)

features_train.shape

features_test.shape

features_val.shape

"""## **Developping Machine Learning Model**

### **Model Trial**

#### **Logistic Regression**
"""

# Define Model

lr = LogisticRegression()
# Training the model

lr.fit(features_train, target_train)

# Training prediction
target_train_pred = lr.predict(features_train)

# Training Result
pd.DataFrame({
    'actual':target_train,
    'prediction': target_train_pred
})

# Training accuracy

accuracy_score(target_train, target_train_pred)

# Validation Prediction
target_val_pred = lr.predict(features_val)

# Validation Accuracy
accuracy_score(target_val, target_val_pred)

"""#### **Decision Tree Model**"""

# Define Model
dt = DecisionTreeClassifier()

# Training The Model
dt.fit(features_train, target_train)

# Training prediction
target_train_pred_dt = dt.predict(features_train)

# Training Result
pd.DataFrame({
    'actual':target_train,
    'prediction': target_train_pred_dt
})

# Training accuracy
accuracy_score(target_train, target_train_pred_dt)

# Validation Prediction
target_train_pred_dt = dt.predict(features_val)

# Validation Accuracy
accuracy_score(target_val, target_train_pred_dt)

"""#### **Random Forest Model**"""

# Define Model
rf = RandomForestClassifier()

# Training The Model
rf.fit(features_train, target_train)

# Training prediction
target_train_pred_rf = rf.predict(features_train)

# Training Result
pd.DataFrame({
    'actual':target_train,
    'prediction': target_train_pred_rf
})

# Training accuracy
accuracy_score(target_train, target_train_pred_rf)

# Validation Prediction
target_train_pred_rf = rf.predict(features_val)

# Validation Accuracy
accuracy_score(target_val, target_train_pred_rf)

"""### **Tuning Hyperparameter**

Tuning process is basically just like training model above but with additional parameter to have detail process of model training. Using hyperparameter we can adjust the n_estimator, or the depth of the tree. Using Random Forest Model we already have 78% of accuracy, but probably we can train better using hyperparameter.

#### **Random Forest Model**

This model chosed because the other two has lower accuracy.
"""

# Define Model and hyperparameter
rf1 = RandomForestClassifier(n_estimators=2000, criterion='gini', max_depth=5)

# Training The Model
rf1.fit(features_train, target_train)

# Training prediction
target_train_pred_rf1 = rf1.predict(features_train)

# Training Result
pd.DataFrame({
    'actual':target_train,
    'prediction': target_train_pred_rf1
})

# Training accuracy
accuracy_score(target_train, target_train_pred_rf1)

# Validation Prediction
target_train_pred_rf1 = rf1.predict(features_val)

# Validation Accuracy
accuracy_score(target_val, target_train_pred_rf1)

"""### **Sanity Check**"""

mean = target.mean()
mean

predictions = pd.Series(target.mean(), index=target.index)
mse = mean_squared_error(target, predictions)
print('MSE:', mse)

rmse = mse ** 0.5

print('RMSE:', rmse)

"""## **Conclusion**

1. The goal of the project is to find model to run with 75% of accuracy for new product.
2. Data must be split for machine learning as below:

- 60% data for training set.
- 20% data for test set.
- 20% data for validation set.

2. We run 3 types of model there are

- Logistic Regression with accuracy of 73.5%
- Decision Tree Model with accuracy of 71.6%
- Random Forest Model with accuracy of 78.2%

3. After we found out that Random Forest Model has higher percentage, lets run tuning hyperparameter on it with parameters below:

- n_estimators=2000
- criterion='gini'
- max_depth=5
- These parameters add up accuracy to 78.3%

4. The sanity check has been done with margin or error of 0.461
5. Therefore the best model to be used is: Random Forest Model with hyperparameters as mentioned in point 3.
"""